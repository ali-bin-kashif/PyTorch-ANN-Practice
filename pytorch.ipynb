{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1001,  1.1000,  0.1073, -0.6133],\n",
       "        [ 0.6674, -0.4449,  0.4817, -1.7785],\n",
       "        [-0.2322,  2.3272, -0.5196,  0.1155]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pytorch Dataset and Dataloaders:**\n",
    "- We ideally want our dataset preparation code to be decoupled from our model training code for better readability and modularity.\n",
    "\n",
    "- PyTorch provides two data primitives which helps us to do this with ease:\n",
    "\n",
    "    - torch.utils.data.DataLoader\n",
    "    - torch.utils.data.Dataset\n",
    "\n",
    "What is Dataset?\n",
    "\n",
    "- **Dataset** stores the samples and their corresponding labels (optionally)\n",
    "\n",
    "What is DataLoader?\n",
    "\n",
    "- **DataLoader** wraps an iterable around the Dataset to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(0, 30)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([3])\n",
      "tensor([4])\n",
      "tensor([5])\n",
      "tensor([6])\n",
      "tensor([7])\n",
      "tensor([8])\n",
      "tensor([9])\n",
      "tensor([10])\n",
      "tensor([11])\n",
      "tensor([12])\n",
      "tensor([13])\n",
      "tensor([14])\n",
      "tensor([15])\n",
      "tensor([16])\n",
      "tensor([17])\n",
      "tensor([18])\n",
      "tensor([19])\n",
      "tensor([20])\n",
      "tensor([21])\n",
      "tensor([22])\n",
      "tensor([23])\n",
      "tensor([24])\n",
      "tensor([25])\n",
      "tensor([26])\n",
      "tensor([27])\n",
      "tensor([28])\n",
      "tensor([29])\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5, 18,  6,  1, 25])\n",
      "tensor([29, 22, 21, 13, 20])\n",
      "tensor([12,  7,  9, 19,  3])\n",
      "tensor([14, 17,  4, 28, 15])\n",
      "tensor([24, 11,  8,  2, 26])\n",
      "tensor([ 0, 16, 10, 23, 27])\n"
     ]
    }
   ],
   "source": [
    "data_2 = DataLoader(X, batch_size=5, shuffle=True)\n",
    "for i in data_2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data loader preparation\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x26d5699bef0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking an sample image , data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(images[2].numpy().squeeze(), cmap='Greys_r')\n",
    "print(labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "We create a new class (which inherits the properties from the base class from nn package called Module) to define the archietecture of the Neural Network.\n",
    "\n",
    "- Layer defination should be inside the constructor of the class.\n",
    "- Forward propagation step should be included inside forward method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activations(Relu,Sigmoid,Tanh etc) and loss functions(cross entropy,nllloss etc) comes from torch.nn.functional module. This module contains all the functions in the torch.nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax of nn.Linear() is (input size, output size)\n",
    "\n",
    "This NN architecture below represents the 784 nodes (28*28 pixels) in the input layer, 256 in the hidden layer, and 10 in the output layer(0-9 numbers). Inside the forward function, we will use the relu activation function in the hidden layer which present under torch.nn.functional module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "super().__init__() is a Python statement that calls the constructor (__init__) of a parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input tensor is flattened \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizer.zero_grad(): - will zero out the gradients from previous traning step , in this way gradients won't be accumulated. This should be done before calculating the gradients at each batch.\n",
    "- criterion(output, target): - we feed in the model predicted values along with actual values to calculate the loss.\n",
    "- optimizer.step(): Once we call loss.backward() , gradients will be calculated and we will use this gradients to update the weights in this step using the learning rate defined in optim.SGD(model.parameters(), lr=0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.5221063167601824\n",
      "Epoch: 2 Training Loss:  0.2740491338695089\n",
      "Epoch: 3 Training Loss:  0.2227673003822565\n",
      "Epoch: 4 Training Loss:  0.18925680502473066\n",
      "Epoch: 5 Training Loss:  0.16469240153674036\n",
      "Epoch: 6 Training Loss:  0.14664381867274642\n",
      "Epoch: 7 Training Loss:  0.13186753585158537\n",
      "Epoch: 8 Training Loss:  0.11954140954185277\n",
      "Epoch: 9 Training Loss:  0.10922725590644404\n",
      "Epoch: 10 Training Loss:  0.10082267535074303\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11): ## run the model for 10 epochs\n",
    "    train_loss = []\n",
    "    \n",
    "    ## training part \n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating the model with F.log_softmax + nn.NLLLoss()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.23836496979929506\n",
      "Epoch: 2 Training Loss:  0.12048680858579852\n",
      "Epoch: 3 Training Loss:  0.09605271527470904\n",
      "Epoch: 4 Training Loss:  0.0804449472046205\n",
      "Epoch: 5 Training Loss:  0.07305528366479848\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6): ## run the model for 5 epochs\n",
    "    train_loss = []\n",
    "    \n",
    "    ## training part \n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4294e+01, -1.9577e+01, -2.3002e+01, -2.5171e+01, -1.1921e-07,\n",
       "         -2.6311e+01, -1.8991e+01, -2.1624e+01, -2.0790e+01, -1.6279e+01]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the class probabilities (softmax) for img\n",
    "log_ps = model(img_test)\n",
    "log_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the class probabilities (softmax) for img\n",
    "ps = torch.exp(log_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8149e-11, 3.1471e-09, 1.0245e-10, 1.1704e-11, 1.0000e+00, 3.7420e-12,\n",
       "         5.6533e-09, 4.0642e-10, 9.3515e-10, 8.5111e-08]],\n",
       "       grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]], grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ps = model(images)\n",
    "ps = torch.exp(log_ps)\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [9],\n",
       "        [4],\n",
       "        [9],\n",
       "        [6],\n",
       "        [8],\n",
       "        [8],\n",
       "        [7],\n",
       "        [2],\n",
       "        [2]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 9, 4, 9, 6, 8, 8, 7, 2, 2, 7, 8, 5, 9, 3, 3, 5, 5, 0, 2, 6, 1, 8, 7,\n",
       "        2, 1, 1, 2, 2, 4, 1, 5, 3, 1, 2, 2, 1, 3, 8, 1, 2, 8, 4, 1, 5, 6, 4, 5,\n",
       "        2, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = top_class == labels.view(*top_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(matches.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
